{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "671cd046",
   "metadata": {},
   "source": [
    "## **Step 1 : Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6553850",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\export\\tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import load_img, img_to_array, array_to_img\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.utils import image_dataset_from_directory\n",
    "from keras.applications import MobileNetV2\n",
    "from keras.applications.mobilenet_v2 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e105c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"RealWaste\"\n",
    "IMG_SIZE = (524, 524)\n",
    "BATCH_SIZE = 32\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275837c2",
   "metadata": {},
   "source": [
    "## **Step 2. Create Train and Validation Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ace0d709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4752 files belonging to 9 classes.\n",
      "Using 3802 files for training.\n",
      "Found 4752 files belonging to 9 classes.\n",
      "Using 950 files for validation.\n"
     ]
    }
   ],
   "source": [
    "train_ds = image_dataset_from_directory(\n",
    "    DATASET_PATH,\n",
    "    validation_split = 0.2,\n",
    "    subset = 'training', \n",
    "    seed = SEED,\n",
    "    image_size = IMG_SIZE,\n",
    "    batch_size = BATCH_SIZE\n",
    ")\n",
    "\n",
    "val_ds = image_dataset_from_directory(\n",
    "    DATASET_PATH,\n",
    "    validation_split = 0.2,\n",
    "    subset = 'validation',\n",
    "    seed = SEED,\n",
    "    image_size = IMG_SIZE,\n",
    "    batch_size = BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d6281e",
   "metadata": {},
   "source": [
    "## **Step 3. Test and Validation data are Same**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea643cd7",
   "metadata": {},
   "source": [
    "## **Step 4. Name and Count Classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fffd74e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Names are: ['Cardboard', 'Food Organics', 'Glass', 'Metal', 'Miscellaneous Trash', 'Paper', 'Plastic', 'Textile Trash', 'Vegetation']\n",
      "Total Number of classes are: 9\n"
     ]
    }
   ],
   "source": [
    "class_names = train_ds.class_names\n",
    "num_classes = len(class_names)\n",
    "\n",
    "print(f\"Class Names are: {class_names}\")\n",
    "print(f\"Total Number of classes are: {num_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107168a2",
   "metadata": {},
   "source": [
    "## **Step 5. Performance Optimization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff92f0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.shuffle(1000).prefetch(buffer_size = AUTOTUNE)\n",
    "val_ds = val_ds.shuffle(1000).prefetch(buffer_size = AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efdf7b7",
   "metadata": {},
   "source": [
    "## **Step 6. Normalization and Augmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d713b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization = layers.Rescaling(1./255)\n",
    "\n",
    "data_augmentation = Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.1),\n",
    "    layers.RandomZoom(0.2)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b5e6e1",
   "metadata": {},
   "source": [
    "## **Step 7. Building the CNN Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a1cc7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_9436\\2401494480.py:1: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  base_model = MobileNetV2(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n",
      "WARNING:tensorflow:From c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_model = MobileNetV2(\n",
    "    input_shape = IMG_SIZE + (3,),\n",
    "    include_top = False,\n",
    "    weights = 'imagenet'\n",
    ")\n",
    "\n",
    "base_model.trainable = False\n",
    "\n",
    "model = Sequential([\n",
    "    layers.Input(shape = IMG_SIZE+(3,)),\n",
    "    data_augmentation,\n",
    "    layers.Lambda(preprocess_input),\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(128, activation = \"relu\"),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(num_classes, activation = \"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de7b8f5",
   "metadata": {},
   "source": [
    "## **Step 8. Build and Compile the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "592d473d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m474s\u001b[0m 4s/step - accuracy: 0.5934 - loss: 1.2012 - val_accuracy: 0.6653 - val_loss: 0.9245\n",
      "Epoch 2/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m637s\u001b[0m 5s/step - accuracy: 0.7409 - loss: 0.7379 - val_accuracy: 0.7579 - val_loss: 0.6952\n",
      "Epoch 3/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m623s\u001b[0m 5s/step - accuracy: 0.7959 - loss: 0.5915 - val_accuracy: 0.7632 - val_loss: 0.6522\n",
      "Epoch 4/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m623s\u001b[0m 5s/step - accuracy: 0.8183 - loss: 0.5186 - val_accuracy: 0.7684 - val_loss: 0.6220\n",
      "Epoch 5/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m606s\u001b[0m 5s/step - accuracy: 0.8509 - loss: 0.4464 - val_accuracy: 0.8158 - val_loss: 0.5115\n",
      "Epoch 6/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m537s\u001b[0m 4s/step - accuracy: 0.8614 - loss: 0.4166 - val_accuracy: 0.8011 - val_loss: 0.5500\n",
      "Epoch 7/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m544s\u001b[0m 4s/step - accuracy: 0.8772 - loss: 0.3648 - val_accuracy: 0.7863 - val_loss: 0.5804\n",
      "Epoch 8/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m467s\u001b[0m 4s/step - accuracy: 0.8795 - loss: 0.3465 - val_accuracy: 0.7905 - val_loss: 0.5591\n",
      "Epoch 9/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m450s\u001b[0m 4s/step - accuracy: 0.8887 - loss: 0.3327 - val_accuracy: 0.8063 - val_loss: 0.5426\n",
      "Epoch 10/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m425s\u001b[0m 3s/step - accuracy: 0.8927 - loss: 0.3092 - val_accuracy: 0.8158 - val_loss: 0.5357\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001),\n",
    "    loss = \"sparse_categorical_crossentropy\",\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b92f9680",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"ecovision_ai_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0668dc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(\"ecovision_ai_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcec6a3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
